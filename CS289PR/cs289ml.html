<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
	<title>Raghu Meka</title>
	<link rel="stylesheet" type="text/css" href="main.css" />
<script type="text/javascript">

        <!--
              function toggle(id)
              {
                  div_el = document.getElementById(id);
                  if (div_el.style.display != 'none')
                  {
                     div_el.style.display='none';
                  }
                  else
                  {
                     div_el.style.display='block';
                  };
              };
          -->
</script>

</head>
<body>
<div class="container">
<div class="container">
<div class="banner">
<ul id="navbar">
<li><a class="nav" href="index.html">Raghu Meka</a></li>
<div style="float:right"><li><a class="nav" href="cs289ml.html">CS289:
  Algorithmic machine learning</a><li></div>
</ul>
</div>
<br>

<div style="float:left; width:800px; margin-left:0em">
  <font size="4"> In this course we will look at core algorithmic aspects
  of machine learning. We will cover several classical tools in
  machine learning but more emphasis will be given to recent
  advances and developing efficient and provable algorithms for
  learning tasks. This is a seminar course: after the initial lectures by me, the students will be expected to present a relevant recent paper in class.
  <br><br>
  <strong>Prerequisites</strong>: An undergraduate course in algorithms (equivalent of CS180) will
  be assumed. The students are expected to be mathematically mature
  and be able to follow and write down formal proofs. Familiarity with probability will be helpful.<br><br>

  <strong>Course work</strong>: We will have three assignments,
  one in-class presentation, and one final project. The
  final project can either be a cohesive literature survey of a
  specific topic, a research project, or an experimental project
  investigating different algorithms on a specific learning
  problem; it can even be in the form of participating in some
  machine learning competitions. The relative weight of the different items will depend on enrollment numbers.<br><br>

    <strong>Resources</strong>: There is no required course text. The following links would be useful:<br>
    Sanjeev Arora's <a href="http://www.cs.princeton.edu/courses/archive/spring15/cos598D/">course</a>.<br>
    Elad Hazan's <a href="http://www.cs.princeton.edu/courses/archive/spring15/cos511/">course</a>.<br>
    Ankur Moitra's <a href="http://people.csail.mit.edu/moitra/409.html">course</a>.<br>
    Draft of <a href="http://www.cs.cornell.edu/jeh/book11April2014.pdf">Foundations of Data Science</a> by Hopcroft and Kannan.<br>
    Links to appropriate papers or other online material (typically other lecture notes) will be provided for each lecture. <br><br>


</font>
<br><br>
<div class="banner">Course Syllabus</div>
<font size="4">The following is a tentative list of topics to be covered.</font>

<div class="paper"><div class="ptitle">Learning theory: what and how? (2 lectures)</div>
<div class="pwho">How to model learning?<br>PAC model<br>Towards
  tractable learning models</div>
  </div>
  
<div class="paper"><div class="ptitle">Linearity: the swiss-army
    knife (3 lectures)</div>
    <div class="pwho">
    Best-fit subspaces, low-rank approximations, and Singular Value Decomposition<br>
    Applications of SVD</div>
    </div>

<div class="paper"><div class="ptitle">Multiplicative weights (2 lectures)</div>
<div class="pwho">Online optimization and regret<br>Applications of multiplicative weights</div>
</div>

<div class="paper"><div class="ptitle">Optimization: the work-horse of learning (3 lectures)</div>
<div class="pwho">Learning as optimization<br>Gradient descent<br>Stochastic gradient descent</div>
  </div>

  <div class="paper"><div class="ptitle">The power of convex
    relaxations (2 lectures)</div>
<div class="pwho">Compressed sensing: formulation and analysis<br>Convexification: matrix completion, sparse PCA</div>
</div>

  <div class="paper"><div class="ptitle">Algorithms for massive data
    problems (2 lectures)</div>
<div class="pwho">Hyperloglog<br>Dimension reduction</div>
</div>

  <div class="paper"><div class="ptitle">Neural networks (2 lectures)</div>
<div class="pwho">Constant-depth circuits, back propogation, and limitations<br>The reemergence of neural nets</div>
</div>

  <div class="paper"><div class="ptitle">Student presentations (4 lectures)</div>
  <div class="pwho">Most topics should be eligible </div>
</div>

</div>
</div>
</div>
</div>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-7001529-2");
pageTracker._trackPageview();
} catch(err) {}</script>
</body>

</html>

