<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
	<title>Raghu Meka</title>
	<link rel="stylesheet" type="text/css" href="main.css" />
<script type="text/javascript">

        <!--
              function toggle(id)
              {
                  div_el = document.getElementById(id);
                  if (div_el.style.display != 'none')
                  {
                     div_el.style.display='none';
                  }
                  else
                  {
                     div_el.style.display='block';
                  };
              };
          -->
</script>

</head>
<body>
<div class="container">
<div class="container">
<div class="banner">
<ul id="navbar">
<li><a class="nav" href="index.html">Raghu Meka</a></li>
<div style="float:right"><li><a class="nav" href="cs289ml.html">CS289ML:
  Algorithmic machine learning</a><li></div>
</ul>
</div>
<br>

<div style="float:left; width:800px; margin-left:0em">
  <font size="4"> In this course we will look at a handful of
  ubiquitous algorithms in machine learning. We will cover several classical tools in
  machine learning but more emphasis will be given to recent
  advances and developing efficient and provable algorithms for
  learning tasks. This is a seminar course: after the initial lectures
  by me, the students will be expected to present a relevant recent
  paper in class. Syllabus can be found below. Here's a detailed schedule:
  <br><br>
  <iframe
  src="https://calendar.google.com/calendar/embed?title=CS289ML%3A%20Algorithmic%20Machine%20Learning&amp;showDate=0&amp;showPrint=0&amp;mode=AGENDA&amp;height=600&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=pkuvrpesilid3jo8lkjvkf1tqg%40group.calendar.google.com&amp;color=%23125A12&amp;ctz=America%2FLos_Angeles"
  style="border-width:0" width="800" height="600" frameborder="0"
  scrolling="no"></iframe>
  <br><br>
  We will use piazza for the course (to ask/answer questions, to post
  announcements, homework etc.,). You can signup <a
  href="http://piazza.com/ucla/winter2016/cs289ml">here</a>. The class
  page is <a href="http://piazza.com/ucla/winter2016/cs289ml/home">here</a>.
  <br><br>
  <strong>Prerequisites</strong>: An undergraduate course in algorithms (equivalent of CS180) will
  be assumed. The students are expected to be mathematically mature
  and be able to follow and write down formal proofs. Familiarity with probability will be helpful.<br><br>

  <strong>Course work</strong>: We will have two assignments,
  one in-class presentation, and one final project. The
  final project can either be a cohesive literature survey of a
  specific topic, a research project, or an experimental project
  investigating different algorithms on a specific learning
  problem; it can even be in the form of participating in some
  machine learning competitions. The relative weight of the different
  items will depend on enrollment numbers.<br><br>

  <strong>Presentation</strong>: Depending on the number of students we will have small
  teams presenting recent research papers in class. The plan for this
  is as follows: 1) I will provide a set of references which you can
  pick from (this is only as a suggestion; outside the list is also
  fine). 2) I will solicit preferences and then assign the topics
  based on everyone's preferences. 3) We will meet a few days before
  the presentation to discuss the content and possibly have a short
  dry run. 4) You will write up a one page summary of the
presentation for other students.<br><br>

<a href="https://docs.google.com/document/d/1hr5PpjOm7vafUy7tPjPtFGTp4C55S9rvXBArvD_8-iY/edit?usp=sharing">Here</a> is a list of suggested topics. We will populate it more as we go along.<br><br>

    <strong>Resources</strong>: There is no required course text. The following links would be useful:<br>
    Sanjeev Arora's <a href="http://www.cs.princeton.edu/courses/archive/spring15/cos598D/">course</a>.<br>
    Elad Hazan's <a href="http://www.cs.princeton.edu/courses/archive/spring15/cos511/">course</a>.<br>
    Ankur Moitra's <a href="http://people.csail.mit.edu/moitra/409.html">course</a>.<br>
    Draft of <a
    href="http://www.cs.cornell.edu/jeh/book11April2014.pdf">Foundations
of Data Science</a> by Hopcroft and Kannan.<br>
    Here are some <a href="gdnotes.pdf">lecture notes</a> on gradient descent. 
    Links to appropriate papers or other online material (typically
  other lecture notes) will be provided for each lecture. <br><br>

  <strong>Hours & Location</strong>: MW 4-5:50, Boelter Hall
  5252. Office hours: Tuesdays 9:30 - 10:30, BH 3732H.<br>


</font>
<br><br>
<div class="banner">Course Syllabus</div>
<font size="4">The following is a tentative list of topics to be covered.</font>

<div class="paper"><div class="ptitle">Learning theory: what and how? (2 lectures)</div>
<div class="pwho">How to model learning?<br>PAC model<br>Towards
  tractable learning models</div>
  </div>
  
<div class="paper"><div class="ptitle">Linearity: the swiss-army
    knife (3 lectures)</div>
    <div class="pwho">
    Best-fit subspaces, low-rank approximations, and Singular Value Decomposition<br>
    Applications of SVD</div>
    </div>

<div class="paper"><div class="ptitle">Multiplicative weights (1 lecture)</div>
<div class="pwho">Online optimization and regret</div>
</div>

<div class="paper"><div class="ptitle">Clustering algorithms (1 lecture)</div>
<div class="pwho">k-means and the EM framework</div>
</div>

<div class="paper"><div class="ptitle">Optimization: the work-horse of learning (3 lectures)</div>
<div class="pwho">Convexity primer<br>Learning as optimization<br>Gradient descent<br>Stochastic gradient descent</div>
  </div>

  <div class="paper"><div class="ptitle">The power of convex
    relaxations (2 lectures)</div>
<div class="pwho">Compressed sensing<br>Convexification: matrix completion, sparse PCA</div>
</div>

  <div class="paper"><div class="ptitle">Neural networks (2 lectures)</div>
<div class="pwho">Constant-depth circuits, back propogation, and limitations<br>The reemergence of neural nets</div>
</div>

  <div class="paper"><div class="ptitle">Student presentations (4 lectures)</div>
  <div class="pwho">Most topics should be eligible </div>
  </div>
<br>
  <strong>Academic honesty</strong>: The students are expected to fully
abide by UCLA's student conduct <a
href="http://www.registrar.ucla.edu/soc/notices.htm">policies</a>,
including Section 102.01 on academic honesty. You will find a wealth
of helpful materials <a
href="http://www.studentgroups.ucla.edu/dos/students/integrity/">here</a>,
including the <a
href="http://www.studentgroups.ucla.edu/dos/assets/documents/StudentGuide.pdf">Student
Guide to Academic Integrity</a>. Academic dishonesty will be promptly
reported to the Dean of Students' Office for adjudication and
disciplinary action. Remember, cheating will have significant and
irrevocable consequences for your academic record and professional
future. Please don't cheat.

<br><br>While collaboration with other students on
assignments is fine, you should clearly mention the collaborators. You
should make your own slides and when you use content from another
source, you should explicitly state so. Under no circumstances may you use code directly from resources on the web without explicitly citing the source.<br><br>

</div>

</div>
</div>
</div>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-7001529-2");
pageTracker._trackPageview();
} catch(err) {}</script>
</body>

</html>

