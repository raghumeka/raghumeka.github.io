<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
	<title>Raghu Meka</title>
	<link rel="stylesheet" type="text/css" href="main.css" />
<script type="text/javascript">

        <!--
              function toggle(id)
              {
                  div_el = document.getElementById(id);
                  if (div_el.style.display != 'none')
                  {
                     div_el.style.display='none';
                  }
                  else
                  {
                     div_el.style.display='block';
                  };
              };
          -->
</script>

</head>
<body>
<div class="container">
<div class="container">
<div class="banner">
<ul id="navbar">
<li><a class="nav" href="index.html">Raghu Meka</a></li>
<div style="float:right"><li><a class="nav" href="cs289ml.html">CS289ML:
  Algorithmic machine learning</a><li></div>
</ul>
</div>
<br>

<div style="float:left; width:800px; margin-left:0em">
  <font size="4"> In this course we will look at a handful of
  ubiquitous algorithms in machine learning. We will cover several classical tools in
  machine learning but more emphasis will be given to recent
  advances and developing efficient and provable algorithms for
  learning tasks. A tentative syllabus/schedule can be found below; the topics
  may change based on student interests as well.
  <br><br>
  <iframe src="https://calendar.google.com/calendar/embed?mode=AGENDA&amp;height=600&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=0ese5edgvh0veqh474voj2j2e0%40group.calendar.google.com&amp;color=%23B1440E&amp;ctz=America%2FLos_Angeles" style="border-width:0" width="800" height="600" frameborder="0" scrolling="no"></iframe>
  <br><br>
  We will use piazza for the course (to ask/answer questions, to post
  announcements, homework etc.,). You can signup <a
  href="http://piazza.com/ucla/fall2016/cs289ml">here</a>. The class
  page is <a href="http://piazza.com/ucla/fall2016/cs289ml/home">here</a>.
  <br><br>
  <strong> Assignments</strong><br>
  <a href="assignment1.pdf">Assignment 1</a>: Due Oct 12.<br>
  <a href="assignment2.pdf">Assignment 2</a>: Due Oct 26.<br>
  <a href="assignment3.pdf">Assignment 3</a>: Due Nov 4, 4PM.<br>
  <a href="assignment4.pdf">Assignment 4</a>: Due Nov 30, 10PM.<br>
  <a href="assignment5.pdf">Assignment 5</a>: Due Dec 7, 10PM.<br><br>
  <strong>Prerequisites:</strong> It is critical to be familiar
  with material from a typical undergraduate course in algorithms
  (equivalent to CS180), and an undergraduate course in linear
  algebra. If you have doubts about this please talk to me right away. Familiarity with probability will be helpful.<br><br>

  <strong>Course work:</strong> We will have five assignments
  (10%x5). Scopes of assignments: 1 - lectures 1-4; 2 - Lectures 5-8; 3 -
  lectures 9-12; 4 - lectures 14-17; 5 - lectures 18-20.<br><br>
  Mid-term - 25%, Nov 7 in class; material from lectures 1 -
  12.<br><br>
  Project - 25%.   <br><br>

 <strong> Assignment submission:</strong> We will use <a
  href="https://gradescope.com">Gradescope</a> for assignments and
  they have to be submitted by 10PM on their due date. This
  is extremely helpful both for me as well as for you -
  you'll get better feedback and will have a digital record of
  all your assignments that you can refer to later. Things to keep in mind: 1) Within a week of the course,
  you should receive a registration link from Gradescope. If you don't
  receive it before the first homework, contact me immediately;
  this will give you access to the website. 2) Watch <a href="https://www.youtube.com/watch?v=-wemznvGPfg">this</a> one-minute
  video with complete instructions. Follow them to the letter! The
  simple guidelines make the process considerably smoother. 3) Make
  sure you start each problem of an assignment on a new page. 4) To
  generate a PDF scan of the assignments, you can follow the instructions
  <a href="submitting_hw_guide.pdf">here</a>; you can also use the
scanners in the library.<br><br>

 <strong> Project:</strong> The final project can either be a cohesive literature survey of a
  specific topic, a research project, or an experimental project
  investigating different algorithms on a specific learning
  problem; it can even be in the form of participating in some
  machine learning competitions. The project will be evaluated on the
  basis of a five page (one-sided) report (Due by December 9th 5PM PST) which is
  expected to be at the level of a conference submission. The project
  can be done in teams of upto three students (the work will have to
  scale accordingly).<br><br>


  <strong>Resources</strong>: There is no required course text. The following links would be useful:<br>
    Sanjeev Arora's <a href="http://www.cs.princeton.edu/courses/archive/spring15/cos598D/">course</a>.<br>
    Elad Hazan's <a href="http://www.cs.princeton.edu/courses/archive/spring15/cos511/">course</a>.<br>
    Ankur Moitra's <a href="http://people.csail.mit.edu/moitra/409.html">course</a>.<br>
    Draft of <a
    href="http://www.cs.cornell.edu/jeh/book2016June9.pdf">Foundations
of Data Science</a> by Hopcroft and Kannan.<br>
    Here are some <a href="gdnotes.pdf">lecture notes</a> on gradient descent. 
    Links to appropriate papers or other online material (typically
  other lecture notes) will be provided for each lecture. <br><br>

  <strong>Hours & Location</strong>: MW 2-3:50, Boelter Hall
  5272. Office hours: Tuesdays 10:30 - 11:30, BH 3732H.<br>


</font>
<br><br>
<div class="banner">Course Syllabus</div>
<font size="4">The following is a tentative list of topics to be covered.</font>

<div class="paper"><div class="ptitle">Learning theory: what and how? (2 lectures)</div>
<div class="pwho">How to model learning?<br>PAC model<br>Towards
  tractable learning models</div>
  </div>
  
<div class="paper"><div class="ptitle">Linearity: the swiss-army
    knife (3 lectures)</div>
    <div class="pwho">
    Best-fit subspaces, low-rank approximations, and Singular Value Decomposition<br>
    Applications of SVD</div>
    </div>

<div class="paper"><div class="ptitle">Multiplicative weights and boosting (2 lectures)</div>
<div class="pwho">Online optimization and regret. Boosting via
  multiplicative weights</div>
</div>

<div class="paper"><div class="ptitle">Optimization: the work-horse of learning (3 lectures)</div>
<div class="pwho">Convexity primer<br>Learning as optimization<br>Gradient descent<br>Stochastic gradient descent</div>
  </div>

  <div class="paper"><div class="ptitle">The power of convex
    relaxations (2 lectures)</div>
<div class="pwho">Compressed sensing<br>Convexification: matrix completion, sparse PCA</div>
</div>

  <div class="paper"><div class="ptitle">Neural networks (2 lectures)</div>
<div class="pwho">Constant-depth circuits, back propogation, and limitations<br>The reemergence of neural nets</div>
</div>

  <div class="paper"><div class="ptitle">Non-negative matrix
    factorization and Topic models (2 lectures)</div>
<div class="pwho">Basic models and algorithms</div>
</div>

  <div class="paper"><div class="ptitle">Algorithmic stability (2 lectures)</div>
<div class="pwho">Stability as a tool for generalization</div>
</div>

  <div class="paper"><div class="ptitle">Independent component
    analysis and sparse coding (1 lecture)</div>
<div class="pwho">ICA model and method of fourth moments</div>
</div>


<br>
  <strong>Academic honesty</strong>: The students are expected to fully
abide by UCLA's student conduct <a
href="http://www.registrar.ucla.edu/soc/notices.htm">policies</a>,
including Section 102.01 on academic honesty. You will find a wealth
of helpful materials <a
href="http://www.studentgroups.ucla.edu/dos/students/integrity/">here</a>,
including the <a
href="http://www.studentgroups.ucla.edu/dos/assets/documents/StudentGuide.pdf">Student
Guide to Academic Integrity</a>. Academic dishonesty will be promptly
reported to the Dean of Students' Office for adjudication and
disciplinary action. Remember, cheating will have significant and
irrevocable consequences for your academic record and professional
future. Please don't cheat.

<br><br>While collaboration with other students on
assignments is fine, you should clearly mention the collaborators. You
should make your own slides and when you use content from another
source, you should explicitly state so. Under no circumstances may you use code directly from resources on the web without explicitly citing the source.<br><br>

</div>

</div>
</div>
</div>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-7001529-2");
pageTracker._trackPageview();
} catch(err) {}</script>
</body>

</html>

